{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASCII\n",
    "American standard cdoe for information interchange\n",
    "Arabic character set\n",
    "\n",
    "Character set limitations prevented languages from talking, Unicode solves this with a uniform character set\n",
    "Unicode.org/charts/\n",
    "\n",
    "to represent wide range of characters computers must handle we represent characters with more than one byte\n",
    "utf-16 - fixed length two bytes\n",
    "utf-32 four bytes\n",
    "utf-8 - one to four bytes\n",
    "    compatible with ascii\n",
    "    recommended for between systems\n",
    "    \n",
    "In python 3, all strings are unicode\n",
    "\n",
    "socket -> recv() -> bytes.decode()\n",
    "str.encode() -> send() -> socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "104\n",
      "True\n",
      "10\n",
      "42\n",
      "108 105 115 116\n"
     ]
    }
   ],
   "source": [
    "print(ord('H'))\n",
    "print(ord('h'))\n",
    "# order is why upper case are less than lower case\n",
    "print('H'<'h')\n",
    "print(ord('\\n'))\n",
    "print(ord('*'))\n",
    "print(ord('l'),ord('i'),ord('s'),ord('t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n",
      "{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}\n",
      "['http://www.dr-chuck.com/page2.htm']\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "hand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "for line in hand :\n",
    "    print(line.decode().strip())\n",
    "    \n",
    "# eats the headers and stores them, can be retrieved\n",
    "# treats the url like a file\n",
    "\n",
    "hand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "counts = dict()\n",
    "for line in hand :\n",
    "    words = line.decode().split()\n",
    "    for word in words :\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "print(counts)\n",
    "\n",
    "# finding links\n",
    "hand = urllib.request.urlopen('http://www.dr-chuck.com/page1.htm')\n",
    "url = list()\n",
    "for line in hand :\n",
    "    line = line.decode()\n",
    "    if not line.startswith('<a href=') :\n",
    "        continue\n",
    "    words = line.split('\"')\n",
    "    url.append(words[1])\n",
    "print(url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Web scraping is pretends to be a browser and retrieves web pages, looks at those pages, extracts info, and then looks for more pages\n",
    "Crawling or spidering the web\n",
    "    monitor a site for new info, make a database for a search engine, get data from a site with no export, who owns the data?\n",
    "biggest issue is parsing the html\n",
    "    syntax errors are common, regexp will be limited\n",
    "    \n",
    "Install BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter: http://www.dr-chuck.com/page1.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.dr-chuck.com/page2.htm\n",
      "[<a href=\"http://www.dr-chuck.com/page2.htm\">\n",
      "Second Page</a>]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = input('Enter:')\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags :\n",
    "    print(tag.get('href', None))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore ssl certificate error\n",
    "import ssl\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.vierfy_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment #1\n",
    "You are to find all the <span> tags in the file and pull out the numbers from the tag and sum the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2670.0\n"
     ]
    }
   ],
   "source": [
    "sampledata = 'http://py4e-data.dr-chuck.net/comments_42.html'\n",
    "data = 'http://py4e-data.dr-chuck.net/comments_284646.html'\n",
    "\n",
    "url = data\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "counts = list()\n",
    "# print(html)\n",
    "# print(soup)\n",
    "# Retrieve all of the anchor tags\n",
    "comments = soup('span')\n",
    "while True :\n",
    "    for comment in comments:\n",
    "    # Look at the parts of a tag\n",
    "        i = float(comment.contents[0])\n",
    "        counts.append(i)\n",
    "    break\n",
    "    \n",
    "Sum = sum(counts) \n",
    "print(Sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program will use urllib to read the HTML from the data files below, extract the href= vaues from the anchor tags, \n",
    "scan for a tag that is in a particular position relative to the first name in the list, \n",
    "follow that link and repeat the process a number of times and report the last name you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sample = 'http://py4e-data.dr-chuck.net/known_by_Fikret.html'\n",
    "actual = 'http://py4e-data.dr-chuck.net/known_by_Robin.html'\n",
    "url = sample\n",
    "num = 7\n",
    "pos = 18\n",
    "\n",
    "if choice.lower() == 'sample' :\n",
    "    url = sample\n",
    "elif choice.lower() == 'actual' :\n",
    "    url = actual\n",
    "    \n",
    "# repeat process i times using range of an integer\n",
    "for i in range(num) :\n",
    "    print(url)\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tags = soup('a')\n",
    "    timer = 0\n",
    "    for tag in tags :\n",
    "        timer += 1\n",
    "        if timer > pos :\n",
    "            break\n",
    "        url = tag.get('href', None)\n",
    "print(url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'http://py4e-data.dr-chuck.net/known_by_Fikret.html'\n",
    "actual = 'http://py4e-data.dr-chuck.net/known_by_Robin.html'\n",
    "url = sample\n",
    "num = 4\n",
    "pos = 3\n",
    "   \n",
    "# repeat process i times using range of an integer\n",
    "for i in range(num) :\n",
    "    print(url)\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tags = soup('a')\n",
    "    timer = 0\n",
    "    for tag in tags :\n",
    "        timer = timer + 1\n",
    "        if timer > pos :\n",
    "            break\n",
    "        url = tag.get('href', None)\n",
    "print(url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://py4e-data.dr-chuck.net/known_by_Anayah.html\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import ssl\n",
    "import re\n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "url = sample\n",
    "#to repeat 18 times#\n",
    "for i in range(4):\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tags = soup('a')\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        count = count +1\n",
    "        #make it stop at position 3#\n",
    "        if count>3:\n",
    "            break\n",
    "        url = tag.get('href', None)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER:\n",
    "    \n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.vierfy_mode = ssl.CERT_NONE\n",
    "\n",
    "sample = 'http://py4e-data.dr-chuck.net/known_by_Fikret.html'\n",
    "actual = 'http://py4e-data.dr-chuck.net/known_by_Robin.html'\n",
    "url = actual\n",
    "num = 7\n",
    "pos = 18\n",
    "   \n",
    "# repeat process i times using range of an integer\n",
    "for i in range(num) :\n",
    "    print(url)\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tags = soup('a')\n",
    "    timer = 0\n",
    "    for tag in tags :\n",
    "        timer = timer + 1\n",
    "        if timer > pos :\n",
    "            break\n",
    "        url = tag.get('href', None)\n",
    "print(url)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
